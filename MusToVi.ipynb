{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "proved-bicycle",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# songs: gnossi_1.mp3, henne_song.mp3, Blumen.wav, Misty.mp3, gnossi_1_full.mp3\n",
    "# Pr채ludium_1_bach.mp3, Pr채ludium_2_Bach.mp3\n",
    "\n",
    "\n",
    "#song_name = \"songs/gnossi_1.mp3\"\n",
    "#song_name = \"songs/henne_song.mp3\"\n",
    "#song_name = \"songs/Pr채ludium_1_bach.mp3\"\n",
    "#song_name = \"songs/Pr채ludium_2_Bach.mp3\"\n",
    "song_name = \"songs/Blumen.wav\"\n",
    "#song_name = \"songs/Misty.mp3\"\n",
    "#song_name = \"songs/space_is_the_place_men_I_trust.mp3\"\n",
    "#song_name = \"songs/opus_men_I_trust.mp3\"\n",
    "\n",
    "# general params\n",
    "fps = 15\n",
    "boost_fps = 60\n",
    "# musicnn params\n",
    "input_overlap = 1 / 30\n",
    "input_length = 3\n",
    "# song params\n",
    "offset = 0\n",
    "duration = None\n",
    "# post processing params\n",
    "upscale = True\n",
    "twice_upscale = False\n",
    "# video params\n",
    "total_effect_strength = 0.45\n",
    "ema_val = 0.3 # 0.99 is too strong\n",
    "ema_val_latent = 0.3\n",
    "sub_steps = 500\n",
    "lpips_weight = 0.7\n",
    "# prompts\n",
    "base_img_path = \"../CLIP_playground/base_images/\"\n",
    "# settings for prompts\n",
    "k = 5\n",
    "prompt_mode = \"top_k\" # top_k, weighted_top_k, gpt\n",
    "taggram_mode = \"feelings\" # full, feelings\n",
    "prefix = \"\"\n",
    "general_theme = \" Trending on artstation.\" #\". In the style of James Gurney.\"\n",
    "create_gpt_artstyle = True\n",
    "create_clusters = True\n",
    "do_create_gpt_cluster_stories = True\n",
    "cluster_k = 8  # number of K-Means clusters to form over all musicnn predictions\n",
    "gpt_story_top_k = 2  # number of k top cluster stories that will be consideren in CLIP guidance\n",
    "img_theme = None # base_img_path + \"hot-dog.jpg\"\n",
    "# base_img_path + \"Autumn_1875_Frederic_Edwin_Church.jpg\"\n",
    "#\". By Pete Mohrbacher.\"\n",
    "#\". In the style of 'The Persistence of Memory' by Dali.\" #\" by madziowa_p.\" #\" by Jan Brueghel the Elder.\"  #\" by Salvador Dali.\" #\" by Greg Rutkowski.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-spouse",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import soundfile\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "resampled_path = \"tmp/resampled.wav\"\n",
    "os.makedirs(\"tmp\", exist_ok=True)\n",
    "\n",
    "# load song and resample to 16k Hz\n",
    "sr = 16000\n",
    "raw_song, old_sr = librosa.load(song_name, offset=offset, duration=duration)\n",
    "song = librosa.resample(raw_song, old_sr, sr)\n",
    "soundfile.write(resampled_path, song, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-supervisor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mustovi_utils import get_taggram\n",
    "    \n",
    "tag_dfs_folder = \"./tmp/tag_dfs\"\n",
    "os.makedirs(tag_dfs_folder, exist_ok=True)\n",
    "key_song_name = song_name.split(\"/\")[-1].split(\".\")[0]\n",
    "tag_df_name = f\"{key_song_name}_{input_length}_{int(1 / input_overlap)}_{offset}_{duration}.csv\"\n",
    "tag_df_path = os.path.join(tag_dfs_folder, tag_df_name)\n",
    "if os.path.exists(tag_df_path):\n",
    "    normed_tag_df = pd.read_csv(tag_df_path, index_col=0)\n",
    "else:\n",
    "    normed_tag_df = get_taggram(resampled_path, input_overlap, input_length)\n",
    "    normed_tag_df.to_csv(tag_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-morrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# alphabetic\n",
    "#plt.figure(figsize=(13, 20))\n",
    "#sns.heatmap(normed_tag_df.T)\n",
    "#plt.tight_layout()\n",
    "# ordered by importance\n",
    "\n",
    "show_all_labels = False\n",
    "\n",
    "if show_all_labels:\n",
    "    plt.figure(figsize=(12, 20))\n",
    "    show_df = normed_tag_df.T.copy()\n",
    "    show_df[\"mean\"] = normed_tag_df.mean(axis=0)\n",
    "    show_df = show_df.sort_values(\"mean\").drop(columns=[\"mean\"])\n",
    "    sns.heatmap(show_df)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruments = [\"violin\", \"strings\", \"sitar\", \"piano\", \"harpsichord\", \n",
    "               \"harp\", \"guitar\", \"drums\", \"flute\", \"synth\", \"cello\"]\n",
    "genres = [\"techno\", \"soul\", \"rock\", \"rnb\", \"punk\", \"pop\", \"opera\", \n",
    "          \"oldies\", \"new age\", \"metal\", \"jazz\", \"indie rock\", \"indie pop\",\n",
    "          \"indie\", \"indian\", \"heavy metal\", \"hard rock\", \"funk\", \"folk\", \n",
    "          \"electronica\", \"electronic\", \"country\", \"classical\", \"classic rock\",\n",
    "          \"classic\", \"choral\", \"blues\", \"alternative rock\", \"alternative\",\n",
    "          \"Progressive rock\", \"House\", \"Hip-hop\"]\n",
    "eras = [\"60s\", \"70s\", \"80s\", \"90s\", \"00s\"]\n",
    "\n",
    "speed_tags = [\"fast\", \"slow\"]\n",
    "feeling_tags = [\"weird\", \"soft\", \"happy\", \"sad\", \"catchy\", \"easy listening\", \"sexy\", \"chillout\", \"beautiful\", \"chill\"]\n",
    "loudness_tags = [\"quiet\", \"loud\"]\n",
    "vibe_tags = [\"ambient\", \"party\", \"dance\", \"Mellow\", \"experimental\"]\n",
    "\n",
    "genre_like_tags = [\"solo\", \"blues\", \"Beat\"]\n",
    "\n",
    "feeling_tags = speed_tags + feeling_tags + loudness_tags + vibe_tags\n",
    "    \n",
    "plt.figure(figsize=(10, 7))\n",
    "show_df = normed_tag_df[feeling_tags].T.copy()\n",
    "show_df[show_df < show_df.mean(axis=0)] = 0\n",
    "show_df[\"mean\"] = show_df.mean(axis=1)\n",
    "show_df = show_df.sort_values(\"mean\").drop(columns=[\"mean\"])\n",
    "sns.heatmap(show_df)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def gpt_create_prompt(model, tokenizer, merged_top_tags, num_beams=6, top_p=0.80, prompt_num=1):\n",
    "    if prompt_num == 0:\n",
    "        prompter = \". Corresponding image description:\"\n",
    "        gpt_prompt = f\"\"\"The following are adjectives describing a song (in order of importance), followed by a description of a corresponding image:\n",
    "    Song description: fast, guitar, sad{prompter} A biker is racing on his motorbike while crying.\n",
    "    Song description: loud, techno, electronic{prompter} An illustration of dynamic and vibrant colors forming geometric shapes.\n",
    "    Song description: {merged_top_tags}{prompter}\"\"\"\n",
    "    if prompt_num == 1:\n",
    "        prompter = \". Full description:\"\n",
    "        gpt_prompt = f\"\"\"The following are adjectives describing a song, followed by a description of the corresponding image:\n",
    "    fast, sad, dark{prompter} An amazing painting of a man running through dark woods while crying.\n",
    "    loud, techno, electronic, abstract{prompter} A breathtaking illustration of dynamic and vibrant colors forming geometric shapes that give the impression of a dance.\n",
    "    slow, soft, beautiful, sad, quiet{prompter} This is a beautiful painting of an old woman who is sitting on a chair with her hands folded in front of her. She is looking at you with a sad expression on her face.\n",
    "    {merged_top_tags}{prompter}\"\"\"\n",
    "    if prompt_num == 2:\n",
    "        prompter = \". Full description:\"\n",
    "        gpt_prompt = f\"\"\"The following are some adjectives, the most important ones are named first, describing a visual artwork. They are followed by a full description of the corresponding artwork:\n",
    "    Adjectives: fast, sad, dark{prompter} An amazing painting of a man running through dark woods while crying.\n",
    "    Adjectives: loud, techno, electronic, abstract{prompter} A breathtaking illustration of dynamic and vibrant colors forming geometric shapes that give the impression of a dance.\n",
    "    Adjectives: slow, soft, beautiful, sad, quiet{prompter} This is a beautiful painting of an old woman who is sitting on a chair with her hands folded in front of her. She is looking at you with a sad expression on her face.\n",
    "    Adjectives: {merged_top_tags}{prompter}\"\"\"\n",
    "    if prompt_num == 3:\n",
    "        prompter = \". Full description:\"\n",
    "        gpt_prompt = f\"\"\"The following are some adjectives, the most important ones are named first, describing a visual artwork. They are followed by a full description of the corresponding artwork:\n",
    "    Adjectives: fast, sad, dark{prompter} An amazing painting of a man running through dark woods while crying. Moody painting.\n",
    "    Adjectives: loud, techno, electronic, abstract{prompter} A breathtaking illustration of dynamic and vibrant colors forming geometric shapes that give the impression of a dance. Rendered in Unreal engine.\n",
    "    Adjectives: slow, soft, beautiful, sad, quiet{prompter} This is a beautiful painting of an old woman who is sitting on a chair with her hands folded in front of her. She is looking at you with a sad expression on her face. Trending on artstation.\n",
    "    Adjectives: {merged_top_tags}{prompter}\"\"\"\n",
    "    if prompt_num == 4:\n",
    "        prompter = \". Full description:\"\n",
    "        gpt_prompt = f\"\"\"The following are adjectives describing a song, followed by a description of the corresponding image:\n",
    "    Adjectives: fast, sad, dark{prompter} An amazing painting of a man running through dark woods while crying.\n",
    "    Adjectives: loud, techno, electronic, abstract{prompter} A breathtaking illustration of dynamic and vibrant colors forming geometric shapes that give the impression of a dance.\n",
    "    Adjectives: slow, soft, beautiful, sad, quiet{prompter} This is a beautiful painting of an old woman who is sitting on a chair with her hands folded in front of her. She is looking at you with a sad expression on her face.\n",
    "    {merged_top_tags}{prompter}\"\"\"\n",
    "    if prompt_num == 5:\n",
    "        prompter = \"This is the full description of the image: \"\n",
    "        gpt_prompt = f\"\"\"These are some adjectives describing an image: {merged_top_tags}. {prompter}\"\"\"\n",
    "    \n",
    "    out = model.generate(tokenizer.encode(gpt_prompt, return_tensors=\"pt\").to(model.device),\n",
    "                         top_p=top_p, do_sample=True, output_scores=True, \n",
    "                         return_dict_in_generate=True, \n",
    "                         max_length=220,\n",
    "                         num_beams=num_beams, \n",
    "                         no_repeat_ngram_size=2,)\n",
    "    out_text = tokenizer.decode(out[\"sequences\"].tolist()[0])\n",
    "    #print(out_text.split(\"Corresponding image description:\")[3:])\n",
    "    clip_prompt = out_text.split(prompter)[-1].split(\"\\n\")[0].strip().strip('<|endoftext|>')\n",
    "    if not clip_prompt.endswith(\".\"):\n",
    "        clip_prompt = \".\".join(clip_prompt.split(\".\")[:-1]) + \".\"\n",
    "    return clip_prompt, out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for theme in themes:\n",
    "#    print(theme)\n",
    "#    out, text = gpt_create_prompt(gpt_model, gpt_tokenizer, \", \".join(theme), num_beams=5, prompt_num=1)\n",
    "#    print(out)\n",
    "#    #print(text)\n",
    "#    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# to test prompts\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "from transformers import GPT2LMHeadModel\n",
    "device = torch.device(\"cuda\")\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "gpt_model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\", pad_token_id=gpt_tokenizer.eos_token_id)\n",
    "#gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "#gpt_model = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=gpt_tokenizer.eos_token_id)\n",
    "gpt_model = gpt_model.to(device)\n",
    "\n",
    "themes = [['chill', 'sexy', 'chillout', 'slow', 'Mellow'],\n",
    " ['slow', 'soft', 'sad', 'beautiful', 'quiet'],\n",
    " ['beautiful', 'sad', 'slow', 'soft', 'Mellow'],\n",
    " ['chillout', 'chill', 'party', 'sexy', 'weird'],\n",
    " ['sexy', 'chill', 'slow', 'chillout', 'happy'],\n",
    " ['quiet', 'catchy', 'soft', 'happy', 'sad'],\n",
    " ['chill', 'slow', 'chillout', 'beautiful', 'Mellow'],\n",
    " ['slow', 'soft', 'beautiful', 'sad', 'quiet']]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-roman",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_create_prompt_3(model, tokenizer, merged_top_tags, num_beams=5):\n",
    "    gpt_prompt = f\"\"\"The words 'sad, beautiful, happy' can be summarized by this sentence:\n",
    "        The beautiful girl walks sadly through the happy crowd.\n",
    "    \n",
    "    The words '{merged_top_tags}' can be summarized by this sentence:\n",
    "        \"\"\"\n",
    "    \n",
    "    out = model.generate(tokenizer.encode(gpt_prompt, return_tensors=\"pt\").to(model.device),\n",
    "                         top_p=0.95, do_sample=True, output_scores=True, \n",
    "                         return_dict_in_generate=True, \n",
    "                         max_length=120,\n",
    "                         num_beams=num_beams, \n",
    "                         no_repeat_ngram_size=2,)\n",
    "    out_text = tokenizer.decode(out[\"sequences\"].tolist()[0])\n",
    "    #print(out_text.split(\"Corresponding image description:\")[3:])\n",
    "    clip_prompt = out_text.split(\"\\n\")[4].split(\"\\n\")[0].strip().strip('<|endoftext|>')\n",
    "    return clip_prompt, out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce taggram to fit fps\n",
    "musicnn_fps = 62.5\n",
    "#averaging_window = int(musicnn_fps / fps) # == 2 - 30fps\n",
    "averaging_window = int(np.round(musicnn_fps / fps)) # == 3 - 20fps\n",
    "# take step average taggram\n",
    "fps_taggram = normed_tag_df.rolling(averaging_window, min_periods=1, axis=0).mean() \n",
    "fps_taggram = fps_taggram.iloc[::averaging_window, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-drink",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide on using only subset\n",
    "used_tag_df = fps_taggram.copy()\n",
    "if taggram_mode == \"feelings\":    \n",
    "    used_tag_df = used_tag_df[feeling_tags]\n",
    "    \n",
    "# rename some columns\n",
    "rename_dict = {\"sexy\": \"sensual\",\n",
    "               \"party\": \"energetic\",\n",
    "               \"dance\": \"moving\",\n",
    "               \"easy listening\": \"harmonious\",\n",
    "               \"catchy\": \"captivating\"}\n",
    "used_tag_df = used_tag_df.rename(columns=rename_dict)\n",
    "tag_df_means = used_tag_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create clusters\n",
    "if create_clusters:\n",
    "    from umap import UMAP\n",
    "    import sklearn\n",
    "    \n",
    "    clustering_feats = used_tag_df.to_numpy()\n",
    "    # add index to give some time continuity\n",
    "    cluster_time = False\n",
    "    if cluster_time:\n",
    "        print(clustering_feats.shape)\n",
    "        idx_arr = np.expand_dims(np.arange(len(clustering_feats)), 1)\n",
    "        idx_arr = idx_arr / idx_arr.std() * 20\n",
    "        clustering_feats = np.concatenate([clustering_feats, idx_arr], axis=1)\n",
    "    # create high dim umap embeddings for clustering\n",
    "    cluster_on_umap_high_d = False\n",
    "    if cluster_on_umap_high_d:\n",
    "        print(\"Umapping...\")\n",
    "        clusterable_embedding = UMAP(\n",
    "            n_neighbors=30,\n",
    "            min_dist=0.0,\n",
    "            n_components=10,\n",
    "            random_state=42,\n",
    "            metric=\"cosine\",\n",
    "        ).fit_transform(clustering_feats)\n",
    "    else:\n",
    "        clusterable_embedding = clustering_feats\n",
    "    \n",
    "    # cluster\n",
    "    import hdbscan\n",
    "    \n",
    "    #from sklearn.metrics.pairwise import pairwise_distances\n",
    "    #distance = pairwise_distances(clusterable_embedding, metric='cosine')\n",
    "        \n",
    "    num_clusters = 0\n",
    "    min_clusters = 3\n",
    "    max_clusters = 8\n",
    "    \n",
    "    min_samples = 3\n",
    "    while max_clusters < num_clusters or num_clusters < min_clusters:\n",
    "        clusterer = hdbscan.HDBSCAN(min_samples=min_samples,\n",
    "                                    min_cluster_size=min_samples, \n",
    "                                    )\n",
    "        labels = clusterer.fit_predict(clusterable_embedding)  \n",
    "        \n",
    "        num_clusters = len(np.unique(labels)) - 1\n",
    "        print(\"Num clusters: \", num_clusters)\n",
    "        print(\"Num outliers: \", (labels == -1).sum())\n",
    "        min_samples += 1\n",
    "    \n",
    "    use_k_means = True\n",
    "    if use_k_means:\n",
    "        clusterer = sklearn.cluster.KMeans(n_clusters=5)\n",
    "        labels = clusterer.fit_predict(clusterable_embedding)\n",
    "        real_centers = clusterer.cluster_centers_\n",
    "\n",
    "        dist_to_centers = np.array([np.mean((emb - real_centers) ** 2, axis=-1)\n",
    "                                    for emb in clusterable_embedding[:, :]])\n",
    "        dist_to_centers = torch.from_numpy(dist_to_centers)\n",
    "\n",
    "        if cluster_time:\n",
    "            centers = real_centers[:, :-1]\n",
    "        else:\n",
    "            centers = real_centers\n",
    "    \n",
    "    show_2d_umap = False\n",
    "    if show_2d_umap:\n",
    "        # create 2D UMAP embedding to plot\n",
    "        mapper = UMAP(\n",
    "            n_neighbors=30,\n",
    "            min_dist=0.0,\n",
    "            n_components=2,\n",
    "            random_state=42,\n",
    "            metric=\"cosine\",\n",
    "        ).fit(clustering_feats)\n",
    "        # make plot\n",
    "        import umap.plot\n",
    "        umap.plot.output_notebook()\n",
    "        df = pd.DataFrame({\"step\": list(range(len(labels))),\n",
    "                           \"cluster\": labels,\n",
    "                           })\n",
    "        p = umap.plot.interactive(mapper, \n",
    "                                  labels=df[\"cluster\"], \n",
    "                                  #values = df[\"step\"],\n",
    "                                  hover_data=df, point_size=10)\n",
    "        umap.plot.show(p)\n",
    "    # show clusters over time\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.scatter(range(len(labels)), labels, s=1.5)\n",
    "    plt.show()\n",
    "    # show heatmap \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    show_df = used_tag_df.T.copy()\n",
    "    show_df[show_df < show_df.mean(axis=0)] = 0\n",
    "    show_df[\"mean\"] = show_df.mean(axis=1)\n",
    "    show_df = show_df.sort_values(\"mean\").drop(columns=[\"mean\"])\n",
    "    sns.heatmap(show_df)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-lighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt_stories_and_weights(cluster_gpt_stories, n_start_prompts, dist_to_centers, gpt_story_top_k, idx):\n",
    "    if cluster_gpt_stories is not None:\n",
    "        story_idx = max(idx - n_start_prompts, 0)\n",
    "        top_k = dist_to_centers[story_idx].topk(k=gpt_story_top_k, largest=False)\n",
    "        story_weights = (1 - (top_k.values / dist_to_centers[story_idx].max())) ** 2\n",
    "        top_idcs = top_k.indices\n",
    "        gpt_stories = [cluster_gpt_stories[i] for i in top_idcs]\n",
    "    else:\n",
    "        gpt_stories = [\"\"]\n",
    "        story_weights = [1]\n",
    "    return gpt_stories, story_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-authorization",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_k_means:\n",
    "    from hdbscan_utils import *\n",
    "\n",
    "    data = clusterable_embedding\n",
    "    \n",
    "    print(\"1\")\n",
    "    tree = clusterer.condensed_tree_\n",
    "    exemplar_dict = {c: exemplars(c, tree) for c in tree._select_clusters()}\n",
    "    cluster_ids = tree._select_clusters()\n",
    "    \n",
    "    raw_tree = tree._raw_tree\n",
    "    all_possible_clusters = np.arange(data.shape[0], raw_tree['parent'].max() + 1).astype(np.float64)\n",
    "    max_lambda_dict = {c:max_lambda_val(c, raw_tree) for c in all_possible_clusters}\n",
    "                       \n",
    "    point_dict = {c:set(points_in_cluster(c, raw_tree)) for c in all_possible_clusters}\n",
    "    \n",
    "    x = 0\n",
    "    membership_vector = combined_membership_vector(x, data, tree, exemplar_dict, cluster_ids,\n",
    "                                                   max_lambda_dict, point_dict, False)\n",
    "    print(membership_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# highest_classification\n",
    "center_df = pd.DataFrame(centers, columns=used_tag_df.columns)\n",
    "\n",
    "cluster_themes = []\n",
    "for i in range(len(center_df)):\n",
    "    cluster_theme = center_df.iloc[i].sort_values(ascending=False)[:5].index.to_list()\n",
    "    cluster_theme = \", \".join(cluster_theme).lower()\n",
    "    print(str(i) + \":\",  cluster_theme)\n",
    "    cluster_themes.append(cluster_theme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_theme = used_tag_df.mean().sort_values(ascending=False)[:5]\n",
    "main_theme_words = \", \".join(main_theme.index.to_list())\n",
    "main_theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main distinctive features \n",
    "print(\", \".join(center_df.std().sort_values(ascending=False)[:5].index.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "# listen to clusters\n",
    "\n",
    "cluster_idx = 0\n",
    "\n",
    "samples_per_step = int(len(song) / len(labels)) + 1\n",
    "\n",
    "frame_assignments = []\n",
    "for label in labels:\n",
    "    frame_assignments.extend([label] * samples_per_step)\n",
    "frame_assignments = np.array(frame_assignments)\n",
    "\n",
    "sections = frame_assignments == cluster_idx\n",
    "\n",
    "song_section = song[sections[:len(song)]]\n",
    "\n",
    "IPython.display.Audio(song_section, rate=sr, autoplay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-packet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def create_gpt_cluster_stories(cluster_words_list, num_stories=10):\n",
    "    from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "    from transformers import GPT2LMHeadModel\n",
    "    device = torch.device(\"cuda\")\n",
    "    gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "    gpt_model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\", pad_token_id=gpt_tokenizer.eos_token_id)\n",
    "    gpt_model = gpt_model.to(device)\n",
    "    \n",
    "    gpt_stories = []\n",
    "    for cluster_words in tqdm(cluster_words_list):\n",
    "        texts = []\n",
    "        for _ in range(num_stories):\n",
    "            out, text = gpt_create_prompt(gpt_model, gpt_tokenizer, cluster_words, num_beams=4, prompt_num=1)\n",
    "            texts.append(out)\n",
    "        gpt_stories.append(texts)\n",
    "    gpt_model = gpt_model.to(\"cpu\")\n",
    "    return gpt_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_gpt_stories = None\n",
    "if do_create_gpt_cluster_stories:\n",
    "    cluster_gpt_stories = create_gpt_cluster_stories(cluster_themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-heavy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test style\n",
    "\"\"\"\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "from transformers import GPT2LMHeadModel\n",
    "device = torch.device(\"cuda\")\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "gpt_model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\", pad_token_id=gpt_tokenizer.eos_token_id)\n",
    "#gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "#gpt_model = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=gpt_tokenizer.eos_token_id)\n",
    "gpt_model = gpt_model.to(device)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_create_style(model, tokenizer, theme_words, num_beams=5, prompt_num=5, top_p=0.8):\n",
    "    if prompt_num == 0:\n",
    "        prompter = \"The name of a matching painter is:\"\n",
    "        #gpt_prompt = f\"\"\"I am an artificial intelligence that can tell you a matching artist (even little known ones) for a set of words describing a painting.\n",
    "        gpt_prompt = f\"\"\"The following are lists of words describing art, followed by the name of the artist:\n",
    "\n",
    "        The art fits these terms: expressionist, beautiful, vibrant. {prompter} Van Gogh.\n",
    "        The art fits these terms: epic, fantasy, stunning, beautiful, moody. {prompter} Greg Rutkowski.\n",
    "        The art fits these terms: realistic, beautiful, landscapes, dinosaurs, forgotten civilizations. {prompter} James Gurney.\n",
    "        The art fits these terms: {theme_words}. {prompter}\"\"\"\n",
    "    if prompt_num == 1:\n",
    "        prompter = \"The name of a matching painter is:\"\n",
    "        gpt_prompt = f\"\"\"The following are lists of words describing art, followed by the name of the artist:\n",
    "\n",
    "        expressionist, beautiful, vibrant. {prompter} Van Gogh.\n",
    "        epic, fantasy, stunning, beautiful, moody. {prompter} Greg Rutkowski.\n",
    "        realistic, beautiful, landscapes, dinosaurs, forgotten civilizations. {prompter} James Gurney.\n",
    "        {theme_words}. {prompter}\"\"\"\n",
    "    elif prompt_num == 2:\n",
    "        prompter = \"Matching visual artist:\"\n",
    "        gpt_prompt = f\"\"\"The following are lists of words describing art, followed by the name of the artist:\n",
    "\n",
    "        expressionist, beautiful, vibrant. {prompter} Van Gogh.\n",
    "        epic, fantasy, stunning, beautiful, moody. {prompter} Greg Rutkowski.\n",
    "        realistic, beautiful, landscapes, forgotten civilizations. {prompter} James Gurney.\n",
    "        {theme_words}. {prompter}\"\"\"\n",
    "    elif prompt_num == 3:\n",
    "        prompter = \"Matching visual artist:\"\n",
    "        gpt_prompt = f\"\"\"The following are lists of words describing art, followed by the name of the artist:\n",
    "\n",
    "        expressionist, beautiful, vibrant. {prompter} Van Gogh.\n",
    "        epic, fantasy, stunning, moody. {prompter} Greg Rutkowski.\n",
    "        realistic, beautiful, landscapes, forgotten civilizations. {prompter} James Gurney.\n",
    "        happy, dreamy, romantic, sensual. {prompter} Gustav Klimt.\n",
    "        {theme_words}. {prompter}\"\"\"\n",
    "    elif prompt_num == 4:\n",
    "        prompter = \". Matching artstyle:\"\n",
    "        gpt_prompt = f\"\"\"The following are lists of adjectives, listed in order of importance. They are followed by a name of an artstyle that matches them:\n",
    "\n",
    "        introspective, beautiful, sad{prompter} Impressionism.\n",
    "        \n",
    "        expressive, wild, colourful{prompter} Expressionism.\n",
    "        \n",
    "        popular, internet{prompter} Trending on artstation.\n",
    "        \n",
    "        rendered, detailed, high-quality{prompter} Rendered in unreal engine.\n",
    "        \n",
    "        {theme_words}{prompter}\"\"\"\n",
    "    elif prompt_num == 5:\n",
    "        prompter = \". Matching artstyle:\"\n",
    "        gpt_prompt = f\"\"\"The following are adjectives, followed by a matching artstyle:\n",
    " realistic, beautiful, landscapes, forgotten civilizations{prompter} By James Gurney.\n",
    " introspective, beautiful, sad{prompter} A moody painting.\n",
    " expressive, wild, colourful{prompter} An expressionist painting.\n",
    " epic, fantasy, stunning, moody{prompter} Illustrated by Greg Rutkowski.\n",
    " {theme_words}{prompter}\"\"\"\n",
    "# happy, dreamy, romantic, sensual{prompter} Painted by Gustav Klimt.\n",
    "# popular, internet{prompter} Trending on artstation.\n",
    "\n",
    "\n",
    "#gpt_prompt = f\"\"\"I am an artificial intelligence that can tell you a matching artist (even little known ones) for a set of words describing a painting.\n",
    "    #The painting fits these terms: {theme_words}. {prompter}\"\"\"\n",
    "    \n",
    "    #prompter = \"He would suggest the artist:\"\n",
    "    #gpt_prompt = f\"\"\"An interview with a world-famous art expert. He can tell you an artist with a matching style for a set of words describing a painting.\n",
    "    #For painting fiting these terms: {theme_words}. {prompter}\"\"\"\n",
    "    \n",
    "    #prompter = \" This painting has been painted by a painter with the name\"\n",
    "    #gpt_prompt = f\"\"\"I would describe this painting as {theme_words}. {prompter}\"\"\"\n",
    "\n",
    "    \n",
    "    out = model.generate(tokenizer.encode(gpt_prompt, return_tensors=\"pt\").to(model.device),\n",
    "                         top_p=0.98, do_sample=True, output_scores=True, \n",
    "                         return_dict_in_generate=True, \n",
    "                         max_length=150,\n",
    "                         num_beams=num_beams, \n",
    "                         no_repeat_ngram_size=2,)\n",
    "    out_text = tokenizer.decode(out[\"sequences\"].tolist()[0])\n",
    "    #print(out_text.split(\"Corresponding image description:\")[3:])\n",
    "    clip_prompt = out_text.split(prompter)[-1].split(\"\\n\")[0].split(\"<|endoftext|>\")[0].strip()\n",
    "    #.split(\".\")[0].strip().strip(\"\\n\")\n",
    "    #while len(clip_prompt) > 25:\n",
    "    #    clip_prompt = \".\".join(clip_prompt.split(\".\")[:-1])\n",
    "    return clip_prompt, out_text#.split(prompter)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-miller",
   "metadata": {},
   "outputs": [],
   "source": [
    "#out = gpt_create_style(gpt_model, gpt_tokenizer, \", \".join(main_theme_words).lower(), num_beams=7, top_p=0.8, prompt_num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-disaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for theme in cluster_themes:\n",
    "#    print(theme)\n",
    "#    out = gpt_create_style(gpt_model, gpt_tokenizer, theme, num_beams=2, top_p=0.8, prompt_num=5)\n",
    "#    print(out[0])\n",
    "#    print()\n",
    "#    theme_dict[theme].append(out[0])\n",
    "\n",
    "#sad_prompt = \"experimental, crazy\"\n",
    "#for i in range(5):\n",
    "#    out = gpt_create_style(gpt_model, gpt_tokenizer, sad_prompt, num_beams=7, top_p=0.7, prompt_num=5)\n",
    "#    print(out[0])\n",
    "#    theme_dict[sad_prompt].append(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-harvest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-mills",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gpt_theme(main_theme_words, num_themes=5):\n",
    "    from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "    from transformers import GPT2LMHeadModel\n",
    "    device = torch.device(\"cuda\")\n",
    "    gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "    gpt_model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\", pad_token_id=gpt_tokenizer.eos_token_id)\n",
    "    #gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    #gpt_model = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=gpt_tokenizer.eos_token_id)\n",
    "    gpt_model = gpt_model.to(device)\n",
    "    prompt_hash_table = dict()\n",
    "    \n",
    "    gpt_themes = []\n",
    "    for _ in range(num_themes):\n",
    "        gpt_theme = \"\"\n",
    "        while len(gpt_theme) < 5:\n",
    "            gpt_theme = gpt_create_style(gpt_model, gpt_tokenizer, main_theme_words, num_beams=5)[0]\n",
    "        gpt_themes.append(gpt_theme)\n",
    "    return gpt_themes\n",
    "\n",
    "gpt_theme = \"\"\n",
    "if  create_gpt_artstyle:\n",
    "    gpt_themes = create_gpt_theme(main_theme_words, num_themes=5)\n",
    "    print(\"GPT themes: \", gpt_themes)\n",
    "    \n",
    "    import torch\n",
    "    torch.cuda.empty_cache()\n",
    "    import gc\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-meaning",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# create musicnn prompts\n",
    "clip_prompts = []\n",
    "pbar = tqdm(list(used_tag_df.iterrows()))\n",
    "\n",
    "for i, row in pbar:\n",
    "    row = row[row > tag_df_means]\n",
    "    sorted_row = row.sort_values(ascending=False)\n",
    "    # generate clip prompt for current musicnn targets\n",
    "    if prompt_mode == \"top_k\":\n",
    "        # get tags\n",
    "        top_tag_names = list(sorted_row.iloc[:k].index)\n",
    "        #print(top_tag_names)\n",
    "        pbar.set_description(\", \".join(top_tag_names))\n",
    "        clip_prompt = \", \".join(top_tag_names)\n",
    "    elif prompt_mode == \"weighted_top_k\":\n",
    "        top_tag_names = list(sorted_row.iloc[:k].index)\n",
    "        top_tag_vals = list(sorted_row.iloc[:k])\n",
    "        clip_prompt = {name: val for name, val in zip(top_tag_names, top_tag_vals)}\n",
    "    elif prompt_mode == \"gpt\":\n",
    "        sorted_row = row.sort_values(ascending=False)\n",
    "        top_tags = sorted_row.iloc[:k]\n",
    "        top_tag_names = list(top_tags.index)\n",
    "        if len(top_tag_names) == 0:\n",
    "            top_tag_names = [\"Undecided emptiness\"]\n",
    "        merged_top_tags = \", \".join(top_tag_names)\n",
    "        if merged_top_tags in prompt_hash_table:\n",
    "            clip_prompt = prompt_hash_table[merged_top_tags]\n",
    "        else:\n",
    "            clip_prompt = gpt_create_prompt(gpt_model, gpt_tokenizer, merged_top_tags)\n",
    "            pbar.set_description(\"Tags: \" + merged_top_tags + \" Prompt: \" + clip_prompt)\n",
    "            #clip_encoding = imagine.create_text_encoding(clip_prompt)\n",
    "            prompt_hash_table[merged_top_tags] = clip_prompt\n",
    "            \n",
    "    clip_prompts.append(clip_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many steps are there to fill at the start of the song (256 is the size of the fft-windows of musicnn)\n",
    "start_prompt = clip_prompts[0]\n",
    "n_start_prompts = int(np.round((len(song) / (256 * averaging_window) - len(used_tag_df))))\n",
    "clip_prompts.extend([start_prompt] * n_start_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "if prompt_mode == \"gpt\":\n",
    "    del gpt_tokenizer\n",
    "    del gpt_model\n",
    "    import torch\n",
    "    torch.cuda.empty_cache()\n",
    "    import gc\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../StyleCLIP_modular\")\n",
    "from style_clip import Imagine, create_text_path\n",
    "import argparse\n",
    "\n",
    "net = \"vqgan\" # conv, vqgan\n",
    "\n",
    "args = {}\n",
    "args[\"lr_schedule\"] = 0\n",
    "args[\"seed\"] = 1\n",
    "\n",
    "args[\"neg_text\"] = None #'incoherent, confusing, cropped, watermarks'\n",
    "#'text, signature, watermarks, writings, scribblings'#\n",
    "\n",
    "args[\"clip_names\"] = [\"ViT-B/16\", \"ViT-B/32\"]#, \"RN50\"]\n",
    "args[\"averaging_weight\"] = 0\n",
    "args[\"early_stopping_steps\"] = 0\n",
    "args[\"use_tv_loss\"] = 1\n",
    "args[\"lpips_weight\"] = lpips_weight\n",
    "\n",
    "if net == \"vqgan\":\n",
    "    args[\"model_type\"] = \"vqgan\"\n",
    "    args[\"lr\"] = 0.1\n",
    "    \n",
    "elif net == \"conv\":\n",
    "    args[\"model_type\"] = \"conv\"\n",
    "    args[\"act_func\"] = \"gelu\"\n",
    "    args[\"stride\"] = 1\n",
    "    args[\"num_layers\"] = 5\n",
    "    args[\"downsample\"] = False\n",
    "    args[\"norm_type\"] = \"layer\"\n",
    "    args[\"num_channels\"] = 64\n",
    "    args[\"sideX\"] = 1080\n",
    "    args[\"sideY\"] = 720\n",
    "    args[\"lr\"] = 0.005\n",
    "    args[\"stack_size\"] = 4\n",
    "\n",
    "\n",
    "args[\"batch_size\"] = 4\n",
    "args[\"sideX\"] = 480\n",
    "args[\"sideY\"] = 272\n",
    "args[\"circular\"] = 0\n",
    "\n",
    "imagine = Imagine(\n",
    "                save_progress=False,\n",
    "                open_folder=False,\n",
    "                save_video=False,\n",
    "                verbose=False,\n",
    "                **args\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_tag_df[used_tag_df < used_tag_df.mean()] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-warning",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "print(\"num start prompts: \", n_start_prompts)\n",
    "print(used_tag_df.iloc[idx].sort_values(ascending=False).iloc[:k])\n",
    "clip_prompts[idx + n_start_prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - dist_to_centers[100] / dist_to_centers[100].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[150:160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 155\n",
    "dists = 1 - dist_to_centers[idx].topk(k=3, largest=False).values / dist_to_centers[idx].max()\n",
    "print(dist_to_centers[idx].topk(k=3, largest=False).indices)\n",
    "dists ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-stress",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_text(text):\n",
    "    #while len(text) < 1000:\n",
    "    #    text = text + text\n",
    "    return [t.cpu().float() for t in imagine.create_text_encoding(text)]\n",
    "\n",
    "def calc_sim(source, target):\n",
    "    sims = []\n",
    "    for i in range(len(target)):\n",
    "        enc = source[i]\n",
    "        targ = target[i]\n",
    "        cosine_sim = torch.cosine_similarity(enc, targ)\n",
    "        sims.append(cosine_sim)\n",
    "    return torch.stack(sims)\n",
    "\n",
    "\n",
    "def get_best(texts, target_text, show_all=False):\n",
    "    theme_enc = enc_text(target_text)\n",
    "    print(\"Theme: \")\n",
    "    print(target_text)\n",
    "    print()\n",
    "    story_encs = []\n",
    "    sims = []\n",
    "    for story in texts:\n",
    "        story_enc = enc_text(story)\n",
    "        sim = calc_sim(story_enc, theme_enc)\n",
    "        sims.append(sim.mean())\n",
    "\n",
    "    topk = torch.stack(sims).topk(k=len(texts))\n",
    "    top_idcs = list(topk.indices)\n",
    "    if show_all:\n",
    "        for story, val in zip(np.array(texts)[top_idcs], topk.values.tolist()):\n",
    "            print(val, story)\n",
    "    used_gpt_story = texts[top_idcs[0]]\n",
    "    print(\"Top story:\")\n",
    "    print(used_gpt_story)\n",
    "    print(topk.values[0])\n",
    "    return used_gpt_story, topk.values[0]\n",
    "\n",
    "# select best gpt stories\n",
    "used_gpt_stories = None\n",
    "if cluster_gpt_stories is not None:\n",
    "    used_gpt_stories = []\n",
    "    for stories, theme in zip(cluster_gpt_stories, cluster_themes):\n",
    "        test_theme = theme\n",
    "        used_gpt_story, score = get_best(stories, theme)\n",
    "        used_gpt_stories.append(used_gpt_story)\n",
    "        \n",
    "# select best gpt theme\n",
    "if create_gpt_artstyle:\n",
    "    gpt_theme, score = get_best(gpt_themes, main_theme_words, show_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate encodings based on prompts\n",
    "\n",
    "clip_target_encodings = []\n",
    "clip_feature_hash_table = dict()\n",
    "gpt_suffix = \"\" if len(gpt_theme) == 0 else f\" In the style of {gpt_theme}\"\n",
    "\n",
    "count = []\n",
    "\n",
    "def encode(prompt):\n",
    "    prompt = prefix + prompt\n",
    "    if general_theme is not None:\n",
    "        prompt = prompt + general_theme\n",
    "    prompt += gpt_suffix\n",
    "    if prompt in clip_feature_hash_table:\n",
    "        encoding = clip_feature_hash_table[prompt]\n",
    "    else:\n",
    "        count.append(0)\n",
    "        #print(prompt)\n",
    "        encoding = imagine.create_clip_encoding(text=prompt, img=img_theme)\n",
    "        #encoding = imagine.create_text_encoding(prompt)\n",
    "        clip_feature_hash_table[prompt] = encoding\n",
    "    return encoding\n",
    "\n",
    "\n",
    "def weighted_average_encoding(encodings, weights):\n",
    "    clip_encoding = [torch.stack([encoding[j] * weight for encoding, weight in zip(encodings, weights)]).sum(dim=0)\n",
    "                         for j in range(len(encodings[0]))]\n",
    "    weight_sum = sum(weights)\n",
    "    clip_encoding = [enc / weight_sum for enc in clip_encoding]\n",
    "    return clip_encoding\n",
    "\n",
    "\n",
    "for idx, prompt in enumerate(tqdm(clip_prompts)):\n",
    "    gpt_stories, story_weights = get_gpt_stories_and_weights(used_gpt_stories, n_start_prompts, \n",
    "                                                             dist_to_centers, gpt_story_top_k, idx)\n",
    "    \n",
    "    story_encodings = []\n",
    "    for gpt_story, story_weight in zip(gpt_stories, story_weights):\n",
    "        if isinstance(prompt, dict):\n",
    "            encodings = []\n",
    "            weights = []\n",
    "            for prompt_key in prompt:\n",
    "                clip_prompt = gpt_story + \" It feels \" + prompt_key + \".\"\n",
    "                encoding = encode(clip_prompt)\n",
    "                encodings.append(encoding)\n",
    "                weights.append(prompt[prompt_key])\n",
    "            clip_encoding = weighted_average_encoding(encodings, weights)\n",
    "        else:\n",
    "            story_prompt = gpt_story + \" \" + prompt + \".\"\n",
    "            clip_encoding = encode(story_prompt)\n",
    "        story_encodings.append(clip_encoding)\n",
    "    clip_encoding = weighted_average_encoding(story_encodings, story_weights)\n",
    "\n",
    "    \n",
    "    clip_encoding = [enc.to(\"cpu\") for enc in clip_encoding]\n",
    "    clip_target_encodings.append(clip_encoding)\n",
    "    \n",
    "print(len(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-experience",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clip_prompts[1000:1010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take ema of encodings to smoothen\n",
    "ema_encodings = []\n",
    "ema = clip_target_encodings[0]\n",
    "for encoding in clip_target_encodings:\n",
    "    ema = [ema_val * ema[i].to(\"cpu\") + (1 - ema_val) * encoding[i].to(\"cpu\") for i in range(len(encoding))]\n",
    "    ema_encodings.append(ema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spec_norm(song):\n",
    "    mel_spec = librosa.feature.melspectrogram(song, sr=16000, S=None, \n",
    "                                              n_fft=512, hop_length=256, \n",
    "                                              win_length=None, window='hann', center=True, \n",
    "                                              pad_mode='reflect', power=2.0)\n",
    "    # Obtain maximum value per time-frame\n",
    "    spec_max = np.amax(mel_spec, axis=0)\n",
    "    #print(spec_max.shape)\n",
    "    # Normalize all values between 0 and 1\n",
    "    mel_spec = (mel_spec - np.min(spec_max)) / np.ptp(spec_max)\n",
    "\n",
    "    #mel_spec = (mel_spec - mel_spec.min()) / (mel_spec.max() - mel_spec.min())\n",
    "    #sns.heatmap(mel_spec[:20, :])\n",
    "    return mel_spec.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-dodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from scipy.interpolate import NearestNDInterpolator\n",
    "from mustovi_utils import get_spec_norm\n",
    "import librosa\n",
    "# create zoom, rotate, shift effects\n",
    "effects = [\"zoom\", \"rotate\", \"shiftX\", \"shiftY\", \"shear\"]\n",
    "harm_effect_dict = {\"rotate\": 0.0}\n",
    "perc_effect_dict = {\"zoom\": -0.5}\n",
    "cqt_effect_dict = [{\"zoom\": 1.0}, \n",
    "                   {\"rotate\": 1.0},\n",
    "                   {\"shiftX\": 1.0}, \n",
    "                   {\"shiftY\": 1.0},\n",
    "                   {\"shiftY\": -1.0},\n",
    "                   {\"shiftX\": -1.0},\n",
    "                   {\"rotate\": -1.0},\n",
    "                   {\"zoom\": -1.0},\n",
    "                  ]\n",
    "# divide song in percussion and harm (might divide in pitches later)\n",
    "song_harm, song_perc = librosa.effects.hpss(song)\n",
    "spec_norm_harm = get_spec_norm(song_harm)\n",
    "spec_norm_perc = get_spec_norm(song_perc)\n",
    "# get cqt spec\n",
    "n_chroma = len(cqt_effect_dict)\n",
    "cqt_spec = librosa.feature.chroma_cqt(y=song, sr=sr,hop_length=256, \n",
    "                                      n_chroma=n_chroma, n_octaves=7, \n",
    "                                      bins_per_octave=n_chroma * 4, norm=None)\n",
    "sns.heatmap(cqt_spec)\n",
    "plt.show()\n",
    "# take window averages to match video fps\n",
    "N = averaging_window\n",
    "spec_norm_harm = np.convolve(spec_norm_harm, np.ones(N) / N , mode='valid')[::N]\n",
    "spec_norm_perc = np.convolve(spec_norm_perc, np.ones(N) /N, mode='valid')[::N]\n",
    "cqt_spec = np.array([np.convolve(cqt_line, np.ones(N) / N, mode='valid')[::N] \n",
    "                     for cqt_line in cqt_spec])\n",
    "# min-max norm\n",
    "spec_norm_harm = (spec_norm_harm - spec_norm_harm.min()) / (spec_norm_harm.max() - spec_norm_harm.min())\n",
    "spec_norm_perc = (spec_norm_perc - spec_norm_perc.min()) / (spec_norm_perc.max() - spec_norm_perc.min())\n",
    "cqt_spec = (cqt_spec - cqt_spec.min()) / (cqt_spec.max() - cqt_spec.min())\n",
    "# create effects\n",
    "class Effect:\n",
    "    def __init__(self, strength, zoom=0, rotate=0, \n",
    "                 shiftX=0, shiftY=0, shear=0):\n",
    "        max_zoom = 0.2\n",
    "        self.zoom = 1 + max_zoom * zoom * strength\n",
    "        max_rotate = 10\n",
    "        self.rotate = max_rotate * rotate * strength\n",
    "        max_shift = 10\n",
    "        self.shift_x = max_shift * shiftX * strength\n",
    "        self.shift_y = max_shift * shiftY * strength\n",
    "        \n",
    "        self.shear = 0\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        # transform it\n",
    "        transformed_img = T.functional.affine(img, \n",
    "                                          angle=self.rotate, \n",
    "                                          translate=(self.shift_x,\n",
    "                                                     self.shift_y), \n",
    "                                          scale=self.zoom, \n",
    "                                          shear=self.shear,\n",
    "                                          fill=0,\n",
    "                                          interpolation=torchvision.transforms.InterpolationMode.BILINEAR\n",
    "                                         )\n",
    "        # fill in zeros with nearest neighbor\n",
    "        data = transformed_img.numpy()\n",
    "        mask = np.where(~(data == 0))\n",
    "        interp = NearestNDInterpolator(np.transpose(mask), data[mask])\n",
    "        image_result = interp(*np.indices(data.shape))\n",
    "        return torch.from_numpy(image_result)\n",
    "        \n",
    "\n",
    "def merge_dicts(effect_dict, effect_strength_dict, amplitude):\n",
    "    for key in effect_strength_dict:\n",
    "        content = effect_strength_dict[key] * amplitude\n",
    "        if key in effect_dict:\n",
    "            effect_dict[key] += content\n",
    "        else:\n",
    "            effect_dict[key] = content\n",
    "\n",
    "\n",
    "effects_list = []\n",
    "for i in range(len(spec_norm_harm)):\n",
    "    harm = spec_norm_harm[i]\n",
    "    perc = spec_norm_perc[i]\n",
    "    cqt = cqt_spec[:, i]\n",
    "    \n",
    "    effect_dict = {}\n",
    "    merge_dicts(effect_dict, harm_effect_dict, harm)\n",
    "    merge_dicts(effect_dict, perc_effect_dict, perc)\n",
    "    for cqt_effect, cqt_amplitude in zip(cqt_effect_dict, cqt):\n",
    "        merge_dicts(effect_dict, cqt_effect, cqt_amplitude)\n",
    "    \n",
    "    effect = Effect(total_effect_strength, **effect_dict)\n",
    "    effects_list.append([effect])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = dist_to_centers[300]\n",
    "dists = 1 - (dists / dists.max())\n",
    "dists = torch.nn.functional.softmax(dists * 40, dim=-1)\n",
    "dists.numpy().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torchvision\n",
    "import tensorflow as tf\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "\n",
    "\n",
    "def sequential_gen(ema_encodings, effects_list):\n",
    "    if len(ema_encodings) > len(effects_list):\n",
    "        ema_encodings = ema_encodings[:-1]\n",
    "    assert len(ema_encodings) == len(effects_list), f\"{len(ema_encodings)}, {len(effects_list)}\"\n",
    "\n",
    "\n",
    "    img_latents = []\n",
    "    img = None\n",
    "    imagine.to(\"cuda\")\n",
    "    imagine.reset()\n",
    "    transformed_img = None\n",
    "\n",
    "    pbar = tqdm(list(range(len(ema_encodings))))\n",
    "    for i in pbar:\n",
    "        clip_encoding, effects = ema_encodings[i], effects_list[i]\n",
    "        # apply effects\n",
    "        if img is not None:\n",
    "            transformed_img = img.float()\n",
    "            for effect in effects:\n",
    "                transformed_img = effect(transformed_img)\n",
    "            transformed_img = transformed_img.cuda()\n",
    "            latent, _, [_, _, indices] = imagine.model.model.model.encode(transformed_img.to(imagine.device).mul(2).sub(1))\n",
    "            imagine.set_latent(latent)\n",
    "        # set target encoding in CLIP\n",
    "        clip_encoding = [part.to(imagine.device) for part in clip_encoding]\n",
    "        imagine.set_clip_encoding(encoding=clip_encoding)\n",
    "        # optimize for some steps\n",
    "        for _ in range(sub_steps):\n",
    "            img, loss = imagine.train_step(0, 0, lpips_img=transformed_img)\n",
    "        img = img.detach().cpu()\n",
    "        # get latent of img\n",
    "        latent = imagine.model.model.get_latent().detach().cpu()\n",
    "        img_latents.append(latent)\n",
    "        # save final img\n",
    "        if i % (len(ema_encodings) // 20) == 0:\n",
    "            pil_img = to_pil(img.squeeze())\n",
    "\n",
    "            display(pil_img)\n",
    "            #clear_output(wait = True)\n",
    "        #pbar.update(1)\n",
    "    return img_latents\n",
    "\n",
    "\n",
    "def unflatten_encodings(encodings, enc_idcs):\n",
    "    return [unflatten_encoding(enc, enc_idcs) for enc in encodings]\n",
    "\n",
    "\n",
    "def unflatten_encoding(encoding, enc_idcs):\n",
    "    return [encoding[idcs[0]:idcs[1]] for idcs in enc_idcs]\n",
    "\n",
    "\n",
    "def get_gpt_stories_and_weights(cluster_gpt_stories, n_start_prompts, dist_to_centers, gpt_story_top_k, idx, t=50):\n",
    "    if cluster_gpt_stories is not None:\n",
    "        story_idx = max(idx - n_start_prompts, 0)\n",
    "        dists = dist_to_centers[story_idx]\n",
    "        dists = 1 - (dists / dists.max())\n",
    "        dists = torch.nn.functional.softmax(dists * t, dim=-1)\n",
    "        top_k = dists.topk(k=gpt_story_top_k, largest=True)\n",
    "        #story_weights = (1 - (top_k.values / dist_to_centers[story_idx].max())) ** 10\n",
    "        story_weights = top_k.values\n",
    "        top_idcs = top_k.indices\n",
    "        gpt_stories = [cluster_gpt_stories[i] for i in top_idcs]\n",
    "    else:\n",
    "        gpt_stories = [\"\"]\n",
    "        story_weights = [1]\n",
    "    return gpt_stories, story_weights\n",
    "\n",
    "\n",
    "def parallel_gen(clip_prompts):\n",
    "    sub_steps = 100\n",
    "    \n",
    "    all_encodings = []\n",
    "    for i, prompt in enumerate(clip_prompts):\n",
    "        gpt_stories, story_weights = get_gpt_stories_and_weights(used_gpt_stories, n_start_prompts, \n",
    "                                                                 dist_to_centers, gpt_story_top_k, i)\n",
    "        story_prompt = gpt_stories[0]\n",
    "        encoding = encode(story_prompt)\n",
    "        all_encodings.append(encoding)\n",
    "        \n",
    "    latent_dict = dict()\n",
    "    \n",
    "    enc_lens = [len(enc) for enc in all_encodings[0]]\n",
    "    enc_idcs = []\n",
    "    last_idx = 0\n",
    "    for l in enc_lens:\n",
    "        enc_idcs.append((last_idx, last_idx + l))\n",
    "        last_idx += l\n",
    "    print(enc_lens)\n",
    "    print(enc_idcs)\n",
    "    print(torch.cat(all_encodings[0]).shape)\n",
    "    flat_targets = [torch.cat(enc).float() for enc in all_encodings]\n",
    "    \n",
    "    unique_targets = torch.unique(torch.stack(flat_targets), dim=0)\n",
    "    print(\"Num unique_targets: \", len(unique_targets))\n",
    "    assert len(unique_targets) < 100\n",
    "    \n",
    "    \n",
    "    target_dict = dict()\n",
    "    img_latents = []\n",
    "    for i, prompt in enumerate(tqdm(clip_prompts)):\n",
    "        gpt_stories, story_weights = get_gpt_stories_and_weights(used_gpt_stories, n_start_prompts, \n",
    "                                                                 dist_to_centers, gpt_story_top_k, i, t=100)\n",
    "        story_prompt = gpt_stories[0]\n",
    "        for story_prompt in gpt_stories:\n",
    "            if story_prompt in target_dict:\n",
    "                latent = target_dict[story_prompt]\n",
    "            else:\n",
    "                print(story_prompt)\n",
    "                encoding = encode(story_prompt)\n",
    "    \n",
    "        #for target in flat_targets:\n",
    "         #   if target in latent_dict:\n",
    "         #       latent = latent_dict[target]\n",
    "         #   else:\n",
    "                imagine.reset()\n",
    "                #unflattend_target = unflatten_encoding(target.half(), enc_idcs)\n",
    "                imagine.set_clip_encoding(encoding=encoding)\n",
    "                for _ in range(sub_steps):\n",
    "                    img, loss = imagine.train_step(0, 0)\n",
    "                latent = imagine.model.model.get_latent().detach().cpu()\n",
    "                target_dict[story_prompt] = latent\n",
    "                pil_img = to_pil(img.squeeze())\n",
    "                display(pil_img)\n",
    "        \n",
    "        #if i % 10 == 0:\n",
    "        #    print(story_weights)\n",
    "        story_encodings = [target_dict[story_prompt] for story_prompt in gpt_stories]\n",
    "        clip_encoding = torch.sum(torch.stack([enc * weight for enc, weight in zip(story_encodings, story_weights)]), dim=0) / sum(story_weights)\n",
    "        img_latents.append(clip_encoding.clone())\n",
    "    \n",
    "    return img_latents\n",
    "\n",
    "img_latents = sequential_gen(ema_encodings, effects_list)\n",
    "#img_latents = parallel_gen(clip_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def slerp(low, high, val):\n",
    "    low_norm = low / torch.norm(low, dim=1, keepdim=True)\n",
    "    high_norm = high / torch.norm(high, dim=1, keepdim=True)\n",
    "    epsilon = 1e-7\n",
    "    omega = (low_norm * high_norm).sum(1)\n",
    "    omega = torch.acos(torch.clamp(omega, -1 + epsilon, 1 - epsilon))\n",
    "    so = torch.sin(omega)\n",
    "    res = (torch.sin((1.0 - val) * omega) / so).unsqueeze(1) * low + (torch.sin(val * omega) / so).unsqueeze(1) * high\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take ema of encodings to smoothen\n",
    "ema_latents = []\n",
    "ema = img_latents[0]\n",
    "for latent in img_latents:\n",
    "    ema = ema_val_latent * ema.to(\"cpu\") + (1 - ema_val) * latent.to(\"cpu\")\n",
    "    ema_latents.append(ema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-partner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate between latents to increase fps and make video smoother\n",
    "goal_frame_count = boost_fps * len(song) / 16000\n",
    "current_frame_count = len(ema_latents)\n",
    "frames_to_add = np.ceil(goal_frame_count / current_frame_count)\n",
    "if frames_to_add > 1:\n",
    "    video_latents = []\n",
    "    for i, latent in enumerate(ema_latents):\n",
    "        if i + 1 == len(ema_latents):\n",
    "            break\n",
    "        else:\n",
    "            next_latent = ema_latents[i + 1]\n",
    "        latents_to_add = [slerp(latent, next_latent, frac) \n",
    "                          for frac in np.arange(frames_to_add) / frames_to_add]\n",
    "        video_latents.extend(latents_to_add)\n",
    "else:\n",
    "    video_latents = ema_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ema_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#torch.save(video_latents, \"video_latents.pt\")\n",
    "#video_latents = torch.load(\"video_latents.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-buying",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "from PIL import Image\n",
    "import soundfile\n",
    "import moviepy.editor as mpy\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def load_img_paths(root):\n",
    "    paths = [os.path.join(root, f) for f in os.listdir(root)\n",
    "        if f.endswith(\".png\") or f.endswith(\".jpg\")]\n",
    "    paths = sorted(paths, key= lambda x: int(x.split(\"/\")[-1].split(\"_\")[0].split(\".\")[0]))\n",
    "    return paths\n",
    "\n",
    "\n",
    "def create_video(imgs, song, vid_name, sr):\n",
    "    if isinstance(imgs, list):\n",
    "        img_len = len(imgs)\n",
    "    else:\n",
    "        img_len = len([img for img in os.listdir(imgs) \n",
    "                       if img.endswith(\".jpg\") or img.endswith(\".png\")])\n",
    "    vid_fps = len(imgs) * 16000 / len(song)\n",
    "\n",
    "    # create paths\n",
    "    date_time = datetime.now().strftime(\"%m_%d_%H:%M\")  #(\"%m/%d/%Y, %H:%M:%S\")\n",
    "    video_path = f\"video_gens/{vid_name}_{args['model_type']}_{date_time}.mp4\"\n",
    "    os.makedirs(\"video_gens\", exist_ok=True)\n",
    "\n",
    "    # Write temporary audio file\n",
    "    soundfile.write('tmp.wav', raw_song, old_sr)\n",
    "\n",
    "    # Generate final video\n",
    "    audio = mpy.AudioFileClip('tmp.wav', fps=old_sr)\n",
    "    video = mpy.ImageSequenceClip(imgs, fps=vid_fps)\n",
    "    video = video.set_audio(audio)\n",
    "    video.write_videofile(video_path, audio_codec='aac', fps=vid_fps)\n",
    "\n",
    "    # Delete temporary audio file\n",
    "    os.remove('tmp.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-today",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear cuda\n",
    "#from numba import cuda\n",
    "#cuda.select_device(0)\n",
    "#cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create latents and save to disk:\n",
    "import torchvision\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "extension = \".jpg\"\n",
    "tmp_folder = \"tmp/vid_imgs\" \n",
    "shutil.rmtree(tmp_folder)\n",
    "os.makedirs(tmp_folder, exist_ok=True)\n",
    "    \n",
    "for i, latent in enumerate(tqdm(video_latents)):\n",
    "    imagine.set_latent(latent)\n",
    "    img = imagine.model.model().to(\"cpu\")\n",
    "    pil_img = to_pil(img.squeeze())\n",
    "    pil_img.save(os.path.join(tmp_folder, f\"{i}{extension}\"), subsampling=0, quality=95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name = f\"{song_name.split('/')[-1].split('.')[0]}_{prompt_mode}_ema{ema_val}_steps{sub_steps}\"\n",
    "video_name += gpt_theme if create_gpt_artstyle else \"\"\n",
    "\n",
    "paths = load_img_paths(tmp_folder)\n",
    "create_video(paths, song, video_name, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-bunny",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Real-ESRGAN and enter the Real-ESRGAN\n",
    "#!git clone https://github.com/xinntao/Real-ESRGAN.git\n",
    "#%cd Real-ESRGAN\n",
    "# Set up the environment\n",
    "#!pip install basicsr\n",
    "#!pip install facexlib\n",
    "#!pip install gfpgan\n",
    "#!pip install -r requirements.txt\n",
    "#!python setup.py develop\n",
    "# Download the pre-trained model\n",
    "#!wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models\n",
    "#!wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth -P experiments/pretrained_models\n",
    "#%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-drink",
   "metadata": {},
   "outputs": [],
   "source": [
    "del imagine\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from realesrgan import RealESRGANer\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.inference_mode()\n",
    "def upscale_imgs(imgs, out_folder=None, scale=4, tile=0):\n",
    "    \n",
    "    model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, \n",
    "                    num_block=23, num_grow_ch=32, scale=scale)\n",
    "    upsampler = RealESRGANer(\n",
    "        scale=scale,\n",
    "        model_path=\"Real-ESRGAN/experiments/pretrained_models/RealESRGAN_x4plus.pth\",\n",
    "        model=model,\n",
    "        tile=tile,\n",
    "        tile_pad=10,\n",
    "        pre_pad=0,\n",
    "        half=1)\n",
    "    \n",
    "    outs = []\n",
    "    for i, img in enumerate(tqdm(imgs)):\n",
    "        if isinstance(img, str):\n",
    "            img = np.array(Image.open(img))[:,:,::-1]\n",
    "        \n",
    "        output, _ = upsampler.enhance(img, outscale=scale)\n",
    "        pil_img = Image.fromarray(output[:,:,::-1])\n",
    "        \n",
    "        if out_folder:\n",
    "            pil_img.save(os.path.join(out_folder, f\"{i}.jpg\"), subsample=0, quality=95)\n",
    "        else:\n",
    "            outs.append(pil_img)\n",
    "    return outs\n",
    "\n",
    "\n",
    "import torchvision\n",
    "\n",
    "to_tensor = torchvision.transforms.ToTensor()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def upscale_imgs_custom(imgs, out_folder=None, scale=4, tile=0):\n",
    "    model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, \n",
    "                    num_block=23, num_grow_ch=32, scale=scale)\n",
    "    loadnet = torch.load(\"Real-ESRGAN/experiments/pretrained_models/RealESRGAN_x4plus.pth\")\n",
    "    if 'params_ema' in loadnet:\n",
    "        keyname = 'params_ema'\n",
    "    else:\n",
    "        keyname = 'params'\n",
    "    model.load_state_dict(loadnet[keyname], strict=True)\n",
    "    model.eval().cuda().half()\n",
    "\n",
    "    outs = []\n",
    "    for i, img in enumerate(tqdm(imgs)):\n",
    "        if isinstance(img, str):\n",
    "            #img = torch.from_numpy(np.ascontiguousarray(Image.open(img))[:,:,::-1].copy()).unsqueeze(0)\n",
    "            img = torch.from_numpy(np.transpose(np.array(Image.open(img))[:,:,::-1].copy(), (2, 0, 1))).float().unsqueeze(0).to(\"cuda\")\n",
    "        \n",
    "        output = model(img.half()).cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "        #output, _ = upsampler.enhance(img, outscale=scale)\n",
    "        output = (output * 255.0).round().astype(np.uint8)\n",
    "        pil_img = Image.fromarray(output[:,:,::-1])\n",
    "        \n",
    "        if out_folder:\n",
    "            pil_img.save(os.path.join(out_folder, f\"{i}.jpg\"), subsample=0, quality=95)\n",
    "        else:\n",
    "            outs.append(pil_img)\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = [input_paths[1000]]\n",
    "#print(path)\n",
    "#upscaled_img = upscale_imgs_custom(path, tile=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-queue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image.open(path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "upscale = True\n",
    "\n",
    "if upscale:\n",
    "    input_folder = \"tmp/vid_imgs\"\n",
    "    output_folder = \"tmp/upscaled_vid_imgs\"\n",
    "    if os.path.exists(output_folder):\n",
    "        shutil.rmtree(output_folder)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    print(\"Upscaling...\")\n",
    "    input_paths = load_img_paths(input_folder)\n",
    "    upscale_imgs(input_paths, out_folder=output_folder, scale=4)\n",
    "    #!CUDA_VISIBLE_DEVICES=0 python Real-ESRGAN/inference_realesrgan.py --model_path Real-ESRGAN/experiments/pretrained_models/RealESRGAN_x4plus.pth --input $input_folder --output $output_folder  --netscale 4 --outscale 4 --half --face_enhance > /dev/null\n",
    "    print(\"Done!\")\n",
    "    # edit name\n",
    "    upscaled_video_name = video_name.split(\"/\")\n",
    "    upscaled_video_name[-1] = \"HD_\" + upscaled_video_name[-1]\n",
    "    upscaled_video_name = \"/\".join(upscaled_video_name)\n",
    "    # create video\n",
    "    paths = load_img_paths(output_folder)\n",
    "    create_video(paths, song, upscaled_video_name, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "if twice_upscale:\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    input_folder = \"tmp/upscaled_vid_imgs\"\n",
    "    output_folder = \"tmp/twice_upscaled_vid_imgs\"\n",
    "    if os.path.exists(output_folder):\n",
    "        shutil.rmtree(output_folder)    \n",
    "    print(\"Upscaling...\")\n",
    "    !CUDA_VISIBLE_DEVICES=0 python Real-ESRGAN/inference_realesrgan.py --model_path Real-ESRGAN/experiments/pretrained_models/RealESRGAN_x2plus.pth --input  $input_folder --output $output_folder  --netscale 2 --outscale 2 --half --face_enhance > /dev/null\n",
    "    print(\"Done!\")\n",
    "    # edit name\n",
    "    twice_upscaled_video_name = upscaled_video_name.split(\"/\")\n",
    "    twice_upscaled_video_name[-1] = \"Full\" + twice_upscaled_video_name[-1]\n",
    "    twice_upscaled_video_name = \"/\".join(twice_upscaled_video_name)\n",
    "    # create video\n",
    "    paths = load_img_paths(output_folder)\n",
    "    create_video(paths, song, twice_upscaled_video_name, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-private",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-surgeon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python inference_realesrgan.py --model_path experiments/pretrained_models/RealESRGAN_x4plus.pth --input \"tmp/upscaled_vid_imgs\" --output \"tmp/twice_upscaled_vid_imgs\"  --netscale 4 --outscale 3.5 --half --face_enhance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-snake",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
